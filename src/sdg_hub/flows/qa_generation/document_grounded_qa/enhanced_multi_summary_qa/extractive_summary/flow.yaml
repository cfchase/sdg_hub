metadata:
  name: "Extractive Summary Knowledge Tuning Dataset Generation Flow"
  description: "Generates training datasets for knowledge tuning by creating diverse question-answer pairs from documents. Uses three summarization strategies (detailed summaries, extractive summaries, key facts) to help LLMs internalize document knowledge, enabling them to answer queries without requiring the original document in context."
  version: "2.0.0"
  author: "SDG Hub Contributors"
  
  recommended_models:
    default: "openai/gpt-oss-120b"
    compatible: ["meta-llama/Llama-3.3-70B-Instruct", "microsoft/phi-4", "mistralai/Mixtral-8x7B-Instruct-v0.1"]
    experimental: []
  
  tags:
    - "knowledge-tuning"
    - "document-internalization"
    - "question-generation"
    - "knowledge-extraction"
    - "qa-pairs"
    - "document-processing"
    - "educational"
    - "multi-strategy-synthesis"
    - "detailed-summaries"
    - "extractive-summaries"
    - "key-facts"
  
  license: "Apache-2.0"
  min_sdg_hub_version: "0.2.0"
  
  dataset_requirements:
    required_columns:
      - "document"
      - "document_outline"
      - "domain"
      - "icl_document"
      - "icl_query_1"
      - "icl_query_2"
      - "icl_query_3"
    description: "Input dataset should contain documents with text content and domain classification. Each document should be substantial enough for meaningful question generation (minimum 100 words recommended). The flow generates three types of summaries: detailed (n=20), extractive (n=10), and key facts (n=50), each producing corresponding QA pairs designed to help LLMs internalize document knowledge for knowledge tuning."

  output_columns:
    - "summary"
    - "question"
    - "response"
    - "raw_document"
    - "relevancy_explanation"
    - "relevancy_score"
    - "faithfulness_explanation"
    - "faithfulness_judgment"
    - "verification_explanation"
    - "verification_rating"

blocks:
  - block_type: DuplicateColumnsBlock
    block_config:
      block_name: duplicate_document_col
      input_cols: {document: base_document}
  
  - block_type: PromptBuilderBlock
    block_config:
      block_name: extractive_summary_prompt
      input_cols: [document, document_outline]
      output_cols: extractive_summary_prompt
      prompt_config_path: extractive_summary.yaml
      format_as_messages: true

  - block_type: LLMChatBlock
    block_config:
      block_name: gen_extractive_summary
      input_cols: extractive_summary_prompt
      output_cols: raw_summary
      max_tokens: 4096
      temperature: 0.7
      n: 50
      async_mode: true

  - block_type: TextParserBlock
    block_config:
      block_name: parse_extractive_summary
      input_cols: raw_summary
      output_cols: summary
      start_tags: [""]
      end_tags: [""]

  - block_type: RenameColumnsBlock
    block_config:
      block_name: rename_to_document_column
      input_cols: {document: raw_document, summary: document}

  - block_type: PromptBuilderBlock
    block_config:
      block_name: question_generation_prompt
      input_cols: [domain, document, document_outline, icl_document, icl_query_1, icl_query_2, icl_query_3]
      output_cols: question_generation_prompt
      prompt_config_path: ../generate_question_list.yaml
      format_as_messages: true

  - block_type: LLMChatBlock
    block_config:
      block_name: question_generation
      input_cols: question_generation_prompt
      output_cols: question_list
      max_tokens: 256
      temperature: 0.7
      n: 1
      async_mode: true

  - block_type: TextParserBlock
    block_config:
      block_name: parse_question_list
      input_cols: question_list
      output_cols: question
      start_tags: ["[QUESTION]"]
      end_tags: ["[END]"]


  - block_type: VerifyQuestionBlock
    block_config:
      block_name: verify_question
      input_cols: [question]
      output_cols: [verification_explanation, verification_rating]
      prompt_config_path: ../../multi_summary_qa/instructlab/evaluate_question.yaml
      filter_value: 1.0
      operation: ge
      convert_dtype: float
      max_tokens: 2048
      async_mode: true
      format_as_messages: true
      start_tags: ["[Start of Explanation]", "[Start of Rating]"]
      end_tags: ["[End of Explanation]", "[End of Rating]"]

  - block_type: PromptBuilderBlock
    block_config:
      block_name: answer_generation_prompt
      input_cols: [question, document, document_outline]
      output_cols: answer_generation_prompt
      prompt_config_path: ../generate_answers.yaml
      format_as_messages: true

  - block_type: LLMChatBlock
    block_config:
      block_name: answer_generation
      input_cols: answer_generation_prompt
      output_cols: response
      max_tokens: 4096
      temperature: 0.7
      n: 1
      async_mode: true

  - block_type: EvaluateFaithfulnessBlock
    block_config:
      block_name: eval_faithfulness
      input_cols: [document, response]
      output_cols: [faithfulness_explanation, faithfulness_judgment]
      prompt_config_path: ../../multi_summary_qa/instructlab/evaluate_faithfulness.yaml
      filter_value: "YES"
      operation: eq
      async_mode: true
      format_as_messages: true
      start_tags: ["[Start of Explanation]", "[Start of Answer]"]
      end_tags: ["[End of Explanation]", "[End of Answer]"]

  - block_type: EvaluateRelevancyBlock
    block_config:
      block_name: eval_relevancy
      input_cols: [question, response]
      output_cols: [relevancy_explanation, relevancy_score]
      prompt_config_path: ../../multi_summary_qa/instructlab/evaluate_relevancy.yaml
      filter_value: 2.0
      operation: eq
      convert_dtype: float
      max_tokens: 2048
      async_mode: true
      format_as_messages: true
      start_tags: ["[Start of Feedback]", "[Start of Score]"]
      end_tags: ["[End of Feedback]", "[End of Score]"]